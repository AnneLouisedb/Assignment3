{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3\n",
    "### By AnneLouise de Boer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The labelling typology for the dataset identifies for each comment a higher-level classification of whether that comment ‘has a place in\n",
    "a healthy online conversation’, accompanied for\n",
    "each comment by binary labels for whether it is:\n",
    "\n",
    "(1) hostile,\n",
    "\n",
    "(2) antagonistic, insulting, provocative or trolling (together, ‘antagonistic’),\n",
    "\n",
    "(3) dismissive\n",
    "\n",
    "(4) condescending or patronising (together, ‘condescending’)\n",
    "\n",
    "(5) sarcastic\n",
    "\n",
    "(6) an unfair generalisation.\n",
    "\n",
    "For each label there is also an associated confidence score (between 0.5 and 1).\n",
    "\n",
    "Trusted judgments column shows how many people have judged the comment. \n",
    "\n",
    "This is a multi-label classification problem.\n",
    "Every label corresponds to a binary classification problem and each comment can belong to more than one label simultaneously. \n",
    "For example, in our case a comment may be hostile, sarcastic and dismissive at the same time.\n",
    "\n",
    "We need to make a multi-labeled model capable of detecting different types of unhealthy like hostile, dismissive, antagonistic etc. We need to create a model which predicts a probability of each type of unhealthy for each comment.\n",
    "\n",
    "https://github.com/kesari007/Toxic-Comment-Classification/blob/master/Minor%202.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train_data_dir = \"https://raw.githubusercontent.com/conversationai/unhealthy-conversations/master/corpus/train.csv\"\n",
    "test_data_dir = \"https://raw.githubusercontent.com/conversationai/unhealthy-conversations/master/corpus/test.csv\"\n",
    "train = pd.read_csv(train_data_dir, index_col = \"_unit_id\")\n",
    "test = pd.read_csv(test_data_dir, index_col = \"_unit_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['_trusted_judgments', 'comment', 'antagonise', 'antagonise:confidence',\n",
      "       'condescending', 'condescending:confidence', 'dismissive',\n",
      "       'dismissive:confidence', 'generalisation', 'generalisation:confidence',\n",
      "       'generalisation_unfair', 'generalisation_unfair:confidence', 'healthy',\n",
      "       'healthy:confidence', 'hostile', 'hostile:confidence', 'sarcastic',\n",
      "       'sarcastic:confidence'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(35503, 18)\n"
     ]
    }
   ],
   "source": [
    "print(train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_trusted_judgments                    int64\n",
      "comment                              object\n",
      "antagonise                            int64\n",
      "antagonise:confidence               float64\n",
      "condescending                         int64\n",
      "condescending:confidence            float64\n",
      "dismissive                            int64\n",
      "dismissive:confidence               float64\n",
      "generalisation                        int64\n",
      "generalisation:confidence           float64\n",
      "generalisation_unfair               float64\n",
      "generalisation_unfair:confidence    float64\n",
      "healthy                               int64\n",
      "healthy:confidence                  float64\n",
      "hostile                               int64\n",
      "hostile:confidence                  float64\n",
      "sarcastic                             int64\n",
      "sarcastic:confidence                float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(train.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(train.isnull().values.any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_trusted_judgments                  False\n",
      "comment                              True\n",
      "antagonise                          False\n",
      "antagonise:confidence               False\n",
      "condescending                       False\n",
      "condescending:confidence            False\n",
      "dismissive                          False\n",
      "dismissive:confidence               False\n",
      "generalisation                      False\n",
      "generalisation:confidence           False\n",
      "generalisation_unfair               False\n",
      "generalisation_unfair:confidence    False\n",
      "healthy                             False\n",
      "healthy:confidence                  False\n",
      "hostile                             False\n",
      "hostile:confidence                  False\n",
      "sarcastic                           False\n",
      "sarcastic:confidence                False\n",
      "dtype: bool\n"
     ]
    }
   ],
   "source": [
    "print(train.isna().any())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Colums that I will use do not contain NaN values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            _trusted_judgments  \\\n",
      "_unit_id                         \n",
      "2297540155                   5   \n",
      "1812168131                   5   \n",
      "1739466190                   3   \n",
      "1739453667                   3   \n",
      "1739459056                   3   \n",
      "...                        ...   \n",
      "2028122744                   5   \n",
      "1739457896                   3   \n",
      "1812167305                   5   \n",
      "1739471203                   3   \n",
      "1739460996                   3   \n",
      "\n",
      "                                                      comment  antagonise  \\\n",
      "_unit_id                                                                    \n",
      "2297540155  Personally I prefer the Flying Spaghetti Monst...           0   \n",
      "1812168131  Your comparing a pipeline to a well? One that ...           0   \n",
      "1739466190  Natives refuse to even consider that their cur...           0   \n",
      "1739453667  Bush could very well win an election today in ...           0   \n",
      "1739459056  Is a Trudeau endorsement OK? After all, he sup...           0   \n",
      "...                                                       ...         ...   \n",
      "2028122744  Step 1 in building a fascist regime: get rid o...           0   \n",
      "1739457896  it takes months for political hate ads to have...           0   \n",
      "1812167305  As long as your hopes for converting the self-...           0   \n",
      "1739471203  but the comments section of wentes columns are...           0   \n",
      "1739460996  Cap and trade. Like the 'net carbon zero' logo...           0   \n",
      "\n",
      "            antagonise:confidence  condescending  condescending:confidence  \\\n",
      "_unit_id                                                                     \n",
      "2297540155                 1.0000              0                    1.0000   \n",
      "1812168131                 1.0000              0                    0.8063   \n",
      "1739466190                 1.0000              0                    1.0000   \n",
      "1739453667                 1.0000              0                    1.0000   \n",
      "1739459056                 1.0000              0                    1.0000   \n",
      "...                           ...            ...                       ...   \n",
      "2028122744                 0.8114              0                    0.7888   \n",
      "1739457896                 1.0000              0                    1.0000   \n",
      "1812167305                 1.0000              0                    1.0000   \n",
      "1739471203                 1.0000              0                    1.0000   \n",
      "1739460996                 1.0000              0                    1.0000   \n",
      "\n",
      "            dismissive  dismissive:confidence  generalisation  \\\n",
      "_unit_id                                                        \n",
      "2297540155           0                 1.0000               0   \n",
      "1812168131           0                 1.0000               0   \n",
      "1739466190           0                 1.0000               0   \n",
      "1739453667           0                 1.0000               0   \n",
      "1739459056           0                 1.0000               0   \n",
      "...                ...                    ...             ...   \n",
      "2028122744           0                 0.8114               0   \n",
      "1739457896           0                 1.0000               0   \n",
      "1812167305           0                 1.0000               0   \n",
      "1739471203           0                 1.0000               0   \n",
      "1739460996           0                 1.0000               0   \n",
      "\n",
      "            generalisation:confidence  generalisation_unfair  \\\n",
      "_unit_id                                                       \n",
      "2297540155                     1.0000                    0.0   \n",
      "1812168131                     1.0000                    0.0   \n",
      "1739466190                     1.0000                    0.0   \n",
      "1739453667                     1.0000                    0.0   \n",
      "1739459056                     1.0000                    0.0   \n",
      "...                               ...                    ...   \n",
      "2028122744                     0.6002                    0.0   \n",
      "1739457896                     1.0000                    0.0   \n",
      "1812167305                     1.0000                    0.0   \n",
      "1739471203                     1.0000                    0.0   \n",
      "1739460996                     1.0000                    0.0   \n",
      "\n",
      "            generalisation_unfair:confidence  healthy  healthy:confidence  \\\n",
      "_unit_id                                                                    \n",
      "2297540155                               1.0        1              0.7981   \n",
      "1812168131                               1.0        1              0.6081   \n",
      "1739466190                               1.0        1              1.0000   \n",
      "1739453667                               1.0        1              1.0000   \n",
      "1739459056                               1.0        1              1.0000   \n",
      "...                                      ...      ...                 ...   \n",
      "2028122744                               0.0        1              0.6125   \n",
      "1739457896                               1.0        1              1.0000   \n",
      "1812167305                               1.0        1              0.7978   \n",
      "1739471203                               1.0        1              1.0000   \n",
      "1739460996                               1.0        1              1.0000   \n",
      "\n",
      "            hostile  hostile:confidence  sarcastic  sarcastic:confidence  \n",
      "_unit_id                                                                  \n",
      "2297540155        0              1.0000          0                1.0000  \n",
      "1812168131        0              1.0000          0                0.8063  \n",
      "1739466190        0              1.0000          0                1.0000  \n",
      "1739453667        0              1.0000          0                1.0000  \n",
      "1739459056        0              1.0000          0                1.0000  \n",
      "...             ...                 ...        ...                   ...  \n",
      "2028122744        0              0.6002          0                0.6002  \n",
      "1739457896        0              1.0000          0                1.0000  \n",
      "1812167305        0              1.0000          0                1.0000  \n",
      "1739471203        0              1.0000          0                1.0000  \n",
      "1739460996        0              1.0000          0                1.0000  \n",
      "\n",
      "[32848 rows x 18 columns]\n"
     ]
    }
   ],
   "source": [
    "print(train[(train[\"healthy\"] == 1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = train[[\"antagonise\" , \"condescending\" , \"dismissive\" , \"generalisation\" , \"generalisation_unfair\" , \"healthy\" , \"hostile\" , \"sarcastic\"]]\n",
    "labels = label.columns\n",
    "confidence_labels = ['antagonise:confidence',\n",
    "                     'condescending:confidence',\n",
    "                     'dismissive:confidence',\n",
    "                     'generalisation:confidence',\n",
    "                     'generalisation_unfair:confidence',\n",
    "                     'hostile:confidence',\n",
    "                     'sarcastic:confidence']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a new dataframes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [],
   "source": [
    "newtrain = pd.DataFrame()\n",
    "for feature in labels:\n",
    "    for value in train[feature]:\n",
    "        if value == 1:\n",
    "            newtrain[\"P1\" + feature] = train[feature+\":confidence\"]\n",
    "            newtrain[\"P0\" + feature] = 1 - train[feature+\":confidence\"]\n",
    "        else:\n",
    "            newtrain[\"P0\" + feature] = train[feature+\":confidence\"]\n",
    "            newtrain[\"P1\" + feature] = 1 - train[feature+\":confidence\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = newtrain.drop(['P0healthy','P1healthy'], axis = 1)\n",
    "y_train = newtrain[[\"P0healthy\",\"P1healthy\"]].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [],
   "source": [
    "newtest = pd.DataFrame()\n",
    "for feature in labels:\n",
    "    for value in test[feature]:\n",
    "        if value == 1:\n",
    "            newtest[\"P1\" + feature] = test[feature+\":confidence\"]\n",
    "            newtest[\"P0\" + feature] = 1 - test[feature+\":confidence\"]\n",
    "        else:\n",
    "            newtest[\"P0\" + feature] = test[feature+\":confidence\"]\n",
    "            newtest[\"P1\" + feature] = 1 - test[feature+\":confidence\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = newtest.drop(['P0healthy','P1healthy'], axis = 1)\n",
    "y_test = newtest[[\"P0healthy\",\"P1healthy\"]].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            P1antagonise  P0antagonise  P1condescending  P0condescending  \\\n",
      "_unit_id                                                                   \n",
      "1739460326        0.4184        0.5816           0.4184           0.5816   \n",
      "2297540155        0.0000        1.0000           0.0000           1.0000   \n",
      "1812168131        0.0000        1.0000           0.1937           0.8063   \n",
      "1739470334        0.2069        0.7931           0.4041           0.5959   \n",
      "1739466190        0.0000        1.0000           0.0000           1.0000   \n",
      "\n",
      "            P1dismissive  P0dismissive  P0generalisation  P1generalisation  \\\n",
      "_unit_id                                                                     \n",
      "1739460326        0.4184        0.5816               1.0               0.0   \n",
      "2297540155        0.0000        1.0000               1.0               0.0   \n",
      "1812168131        0.0000        1.0000               1.0               0.0   \n",
      "1739470334        0.4041        0.5959               1.0               0.0   \n",
      "1739466190        0.0000        1.0000               1.0               0.0   \n",
      "\n",
      "            P0generalisation_unfair  P1generalisation_unfair  P1hostile  \\\n",
      "_unit_id                                                                  \n",
      "1739460326                      1.0                      0.0     0.4184   \n",
      "2297540155                      1.0                      0.0     0.0000   \n",
      "1812168131                      1.0                      0.0     0.0000   \n",
      "1739470334                      1.0                      0.0     0.0000   \n",
      "1739466190                      1.0                      0.0     0.0000   \n",
      "\n",
      "            P0hostile  P0sarcastic  P1sarcastic  \n",
      "_unit_id                                         \n",
      "1739460326     0.5816       0.8001       0.1999  \n",
      "2297540155     1.0000       1.0000       0.0000  \n",
      "1812168131     1.0000       0.8063       0.1937  \n",
      "1739470334     1.0000       0.6052       0.3948  \n",
      "1739466190     1.0000       1.0000       0.0000  \n"
     ]
    }
   ],
   "source": [
    "P1_labels = [\"P1antagonise\", \"P1condescending\", \n",
    "             \"P1dismissive\", \"P1generalisation\",\n",
    "             \"P1generalisation_unfair\", \"P1hostile\",\n",
    "             \"P1sarcastic\"]\n",
    "P1_labels_and_P1healthy = [\"P1healthy\", \"P1antagonise\", \"P1condescending\", \n",
    "             \"P1dismissive\", \"P1generalisation\",\n",
    "             \"P1generalisation_unfair\", \"P1hostile\",\n",
    "             \"P1sarcastic\"]\n",
    "print(x_train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            P0healthy  P1healthy\n",
      "_unit_id                        \n",
      "1739460326     0.4184     0.5816\n",
      "2297540155     0.2019     0.7981\n",
      "1812168131     0.3919     0.6081\n",
      "1739470334     0.2083     0.7917\n",
      "1739466190     0.0000     1.0000\n",
      "...               ...        ...\n",
      "2028122744     0.3875     0.6125\n",
      "1739457896     0.0000     1.0000\n",
      "1812167305     0.2022     0.7978\n",
      "1739471203     0.0000     1.0000\n",
      "1739460996     0.0000     1.0000\n",
      "\n",
      "[35503 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Looking at labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f961fb0b1c0>"
      ]
     },
     "execution_count": 419,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "corrtrain = newtrain[P1_labels_and_P1healthy]\n",
    "corr= corrtrain.corr()\n",
    "plt.figure(figsize=(10,8))\n",
    "sns.heatmap(corr,\n",
    "            xticklabels=corr.columns.values,\n",
    "            yticklabels=corr.columns.values, annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a negative correlation with healthy..\n",
    "\n",
    "\n",
    "Reading:\n",
    "https://towardsdatascience.com/understanding-binary-cross-entropy-log-loss-a-visual-explanation-a3ac6025181a\n",
    "\n",
    "p0: probability that the label is 0.\n",
    "\n",
    "p1: probability that the label is 1.\n",
    "\n",
    "cross entropy loss compares the prediction from the classifier to the distribution\n",
    "label is 0, confidence is 0.8. Then (0.8, 0.2)\n",
    "compute the cross-entropy of the classifiers prediction with respect to the distribution (0.2, 0.8).\n",
    "The sum over all the instances of the training set will be the adjusted loss function.\n",
    "\n",
    "Log loss:\n",
    "y is the label (1 for healthy and 0 for unhealthy) and p(y) is the predicted probability\n",
    "of the comment being healthy for all N comments.\n",
    "\n",
    "Reading:\n",
    "https://datascience.stackexchange.com/questions/19186/can-training-label-confidence-be-used-to-improve-prediction-accuracy\n",
    "\n",
    "## Cross entropy as loss function\n",
    "\n",
    "cross entropy between two probability distributions H(P,Q) = - sum x in XP(x)*log(Q(x))\n",
    "P(x) is the probability of the event x in P.\n",
    "Q(x) is the probability of the event x in Q.\n",
    "log is the base-2 logarithm\n",
    "\n",
    "Reading:\n",
    "https://machinelearningmastery.com/how-to-score-probability-predictions-in-python/\n",
    "\n",
    "Log loss, or cross entropy:\n",
    "Every predicted probability is compared to the actual class output value (0 or 1) and a score is calculated that penalizes the probability based on the distance from the expected value.\n",
    "The penalty is logarithmic, offering a small score for small differences (0.1 or 0.2) and enormous score\n",
    "for a large difference (0.9 or 1.0).\n",
    "Thus, a perfect model has a log loss score of nearly 0.00. (due to the log)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://machinelearningmastery.com/loss-and-loss-functions-for-training-deep-learning-neural-networks/\n",
    "\"The Python function below provides a pseudocode-like working implementation of a function for calculating the cross-entropy for a list of actual 0 and 1 values compared to predicted probabilities for the class 1.\"\n",
    "\n",
    "\"Cross-entropy can be calculated for multiple-class classification. The classes have been one hot encoded, meaning that there is a binary feature for each class value and the predictions must have predicted probabilities for each of the classes. The cross-entropy is then summed across each binary feature and averaged across all examples in the dataset.\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://en.wikipedia.org/wiki/Multi-label_classification\n",
    "\n",
    "Wikipedia provides a clear explaination of the multi-label classification. The 3 transformations are the following:\n",
    "1. Transform to Binary classification\n",
    "\n",
    "Training one binary classifier for each label. We are dividing the task into multiple binary tasks, the single classifiers deal with a single label, labels are predicted sequentially and the outputs of previous classifiers are input as features to subsequent classifiers.\n",
    "2. Transform to multi-class classification problem\n",
    "\n",
    "The label powerset (LP) transformation creates one binary classifier for every label combination present in the training set. \n",
    "3. Ensemble methods\n",
    "\n",
    "A set of multi-class classifiers can be used to create a multi-label ensemble classifier. Each classifier outputs a single class (one label in the multi-label problem) These predictions are then combined by an ensemble method. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Transform to Binary Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data set is unbalanced because the number of healthy and unhealthy comments are unbalanced. In this case binary_crossentropy is the loss function because it independently optimises each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train[P1_labels]  #confidence values of the P1 labels\n",
    "x_test = x_test[P1_labels]   #confidence values of the P1 labels\n",
    "ytrain_sampleweight = train[\"healthy:confidence\"].values #confidence label\n",
    "ytest_sampleweight = test[\"healthy:confidence\"].values #confidence label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_predict, cross_val_score\n",
    "from sklearn.metrics import roc_auc_score, roc_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_train and y_train for the classifiers\n",
    "y_train = train[\"healthy\"].values #class values 0 and 1\n",
    "x_test = x_test\n",
    "y_test = test[\"healthy\"].values   #class values 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 1 ... 1 1 1]\n",
      "[1 1 1 ... 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "print(y_train)\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35503"
      ]
     },
     "execution_count": 424,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32848"
      ]
     },
     "execution_count": 425,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_train[y_train == 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2655"
      ]
     },
     "execution_count": 426,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_train[y_train ==0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4425"
      ]
     },
     "execution_count": 427,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4105"
      ]
     },
     "execution_count": 428,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_test[y_test ==1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "320"
      ]
     },
     "execution_count": 429,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_test[y_test ==0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that this is a highly inbalanced dataset, where in most cases the comment is healthy. We are tyring to detect unhealthy comments, so we could end up with a bad model with high accuracy. I will therefore use the F1 score instead of accuracy to measure the performance of my model. For imbalanced classification there are two main approaches to random resampling; they are oversampling and undersampling.\n",
    "\n",
    "\n",
    "https://machinelearningmastery.com/tactics-to-combat-imbalanced-classes-in-your-machine-learning-dataset/\n",
    "\n",
    "'Confusion Matrix: A breakdown of predictions into a table showing correct predictions (the diagonal) and the types of incorrect predictions made (what classes incorrect predictions were assigned).\n",
    "\n",
    "Precision: A measure of a classifiers exactness.\n",
    "\n",
    "Recall: A measure of a classifiers completeness\n",
    "\n",
    "F1 Score (or F-score): A weighted average of precision and recall.\n",
    "\n",
    "ROC Curves: Like precision and recall, accuracy is divided into sensitivity and specificity and models can be chosen based on the balance thresholds of these values.\"\n",
    "\n",
    "## Random Oversampling Imbalanced dataset\n",
    "Random Oversampling: Randomly duplicate examples in the minority class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imblearn as imb\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "\n",
    "# define oversampling strategy\n",
    "oversample = RandomOverSampler(sampling_strategy=0.5)\n",
    "# fit and apply the transform\n",
    "x_over, y_over = oversample.fit_resample(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({1: 32848, 0: 2655})\n",
      "Counter({1: 32848, 0: 16424})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# summarize class distribution\n",
    "print(Counter(y_train))\n",
    "print(Counter(y_over))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This added the amount of 50% of the majority class (of random duplicated of the minority class) to the minority class. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random undersampling for Imbalanced Dataset\n",
    "Random Undersampling: Randomly delete examples in the majority class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import NearMiss\n",
    "\n",
    "#implementing undersampling for handeling imbalanced data\n",
    "nm = NearMiss()\n",
    "x_under, y_under = nm.fit_sample(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5310, 7)"
      ]
     },
     "execution_count": 433,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_under.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5310,)\n",
      "Counter({1: 32848, 0: 2655})\n",
      "Counter({0: 2655, 1: 2655})\n"
     ]
    }
   ],
   "source": [
    "print(y_under.shape)\n",
    "print(Counter(y_train))\n",
    "print(Counter(y_under))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining Oversampling and Undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "# define oversampling strategy\n",
    "over = RandomOverSampler(sampling_strategy=0.5)\n",
    "# fit and apply the transform\n",
    "x, y = over.fit_resample(x_train, y_train)\n",
    "# define undersampling strategy\n",
    "under = RandomUnderSampler(sampling_strategy=0.5)\n",
    "# fit and apply the transform\n",
    "x_cb, y_cb = under.fit_resample(x, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://machinelearningmastery.com/tactics-to-combat-imbalanced-classes-in-your-machine-learning-dataset/\n",
    "\n",
    "# Decision Tree classifier\n",
    "Using a decision tree classifier, I will show the different results using undersampling, oversampling and a combination of the two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 score decision tree training data): 0.95409278600923\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 119,  201],\n",
       "       [ 177, 3928]])"
      ]
     },
     "execution_count": 436,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dt = DecisionTreeClassifier()\n",
    "dt.fit(x_train,y_train)\n",
    "\n",
    "y_pred = dt.predict(x_test)\n",
    "print(f\"f1 score decision tree training data): {f1_score(y_test, y_pred)}\")\n",
    "#making a confusion matrix\n",
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Undersampling, Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In undersampling we are losing data, so i predcit that the model will work better using oversampling. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 score decision tree (undersampling): 0.7688629911848199\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 305,   15],\n",
       "       [1532, 2573]])"
      ]
     },
     "execution_count": 437,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, mean_squared_error \n",
    "dt.fit(x_under,y_under)\n",
    "y_pred = dt.predict(x_test)\n",
    "\n",
    "print(f\"f1 score decision tree (undersampling): {f1_score(y_test, y_pred)}\")\n",
    "#making a confusion matrix\n",
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With undersampling we are still getting a lot of false negatives. As you can see below, oversampling does indeed score better than undersampling. \n",
    "\n",
    "\n",
    "## Oversampling, Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 score decision tree (oversampling)): 0.949317738791423\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 113,  207],\n",
       "       [ 209, 3896]])"
      ]
     },
     "execution_count": 438,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, mean_squared_error, f1_score \n",
    "\n",
    "dt.fit(x_over,y_over)\n",
    "y_pred = dt.predict(x_test)\n",
    "\n",
    "print(f\"f1 score decision tree (oversampling)): {f1_score(y_test, y_pred)}\")\n",
    "#making a confusion matrix\n",
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The confusion matrix of the oversampled data looks better than the undersampled one. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining Oversampling and Undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({1: 32848, 0: 2655})\n",
      "Counter({1: 32848, 0: 16424})\n"
     ]
    }
   ],
   "source": [
    "print(Counter(y_train))\n",
    "print(Counter(y_cb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 score (forest): 0.9530054644808743\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 114,  206],\n",
       "       [ 181, 3924]])"
      ]
     },
     "execution_count": 440,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt.fit(x_cb,y_cb)\n",
    "y_pred = dt.predict(x_test)\n",
    "\n",
    "print(f\"f1 score (forest): {f1_score(y_test, y_pred)}\")\n",
    "#making a confusion matrix\n",
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I tried oversampling and undersampling and a combination of both, but this did not get significantly better results compared just using the training set. \n",
    "\n",
    "Boosting and bagging the Decision Tree classifier to reduce the accuracy score. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [
    {
     "ename": "TerminatedWorkerError",
     "evalue": "A worker process managed by the executor was unexpectedly terminated. This could be caused by a segmentation fault while calling the function or by an excessive memory usage causing the Operating System to kill the worker.\n\nThe exit codes of the workers are {SIGABRT(-6), SIGABRT(-6), SIGABRT(-6)}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTerminatedWorkerError\u001b[0m                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-441-0c1e8e73ea49>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m )\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mbag_clf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbag_clf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mf1_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/ensemble/_bagging.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    241\u001b[0m         \u001b[0mself\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m         \"\"\"\n\u001b[0;32m--> 243\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    244\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_parallel_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/ensemble/_bagging.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, max_samples, max_depth, sample_weight)\u001b[0m\n\u001b[1;32m    367\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_seeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mseeds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 369\u001b[0;31m         all_results = Parallel(n_jobs=n_jobs, verbose=self.verbose,\n\u001b[0m\u001b[1;32m    370\u001b[0m                                \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parallel_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m             delayed(_parallel_build_estimators)(\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1042\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1043\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1044\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    919\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 921\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    922\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    540\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[1;32m    541\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 542\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    543\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mCfTimeoutError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    437\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mCancelledError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mFINISHED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 439\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    440\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/concurrent/futures/_base.py\u001b[0m in \u001b[0;36m__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    386\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 388\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    389\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTerminatedWorkerError\u001b[0m: A worker process managed by the executor was unexpectedly terminated. This could be caused by a segmentation fault while calling the function or by an excessive memory usage causing the Operating System to kill the worker.\n\nThe exit codes of the workers are {SIGABRT(-6), SIGABRT(-6), SIGABRT(-6)}"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "#defining bagging classifier\n",
    "bag_clf = BaggingClassifier(\n",
    "    DecisionTreeClassifier(),\n",
    "    n_estimators=500,\n",
    "    max_samples=100,\n",
    "    bootstrap=True,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "bag_clf.fit(x_train, y_train)\n",
    "y_pred = bag_clf.predict(x_test)\n",
    "f1_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Boosting - Ada Boost\n",
    "from sklearn. ensemble import RandomForestClassifier, BaggingClassifier, AdaBoostClassifier, VotingClassifier\n",
    "\n",
    "adb = AdaBoostClassifier(DecisionTreeClassifier())\n",
    "adb.fit(x_train,y_train)\n",
    "\n",
    "\n",
    "print(adb.score(x_test,y_test))\n",
    "\n",
    "print(adb.score(x_train,y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Classifier\n",
    "fit(X, y[, sample_weight])\n",
    "I want use the confidence score as a sample_weight for each binary value in the y_train. If people give a 0.51 score for healthy, that means something differnt than a 0.9 score for example. So output with higher confidence scores weigh more. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 score (forest): 0.973328046073677\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[  97,  223],\n",
       "       [  76, 4029]])"
      ]
     },
     "execution_count": 456,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Random Forest - Ensemble of Descision Trees\n",
    "from sklearn. ensemble import RandomForestClassifier, BaggingClassifier, AdaBoostClassifier, VotingClassifier\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error, f1_score \n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=100,random_state=42)\n",
    "rf.fit(x_train,y_train,sample_weight= ytrain_sampleweight)\n",
    "\n",
    "y_pred = rf.predict(x_test)\n",
    "print(f\"f1 score (forest): {f1_score(y_test, y_pred,sample_weight= ytest_sampleweight)}\")\n",
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see I used sample weights for fitting and scoring. The f1 score improved from 0.963 to 0.97. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 60 candidates, totalling 300 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = { \n",
    "    'n_estimators': [200, 500],\n",
    "    'max_features': ['auto', 'sqrt', 'log2'],\n",
    "    'max_depth' : [4,5,6,7,8],\n",
    "    'criterion' :['gini', 'entropy']\n",
    "}\n",
    "CV_rfc = GridSearchCV(estimator=rf, param_grid=param_grid, cv= 5, verbose = 1, scoring='f1_micro')\n",
    "CV_rfc.fit(x_train, y_train,sample_weight= ytrain_sampleweight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CV_rfc.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The confusion matrix shows that there are: \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want to find the difference between predicted values and true confidence score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It has one column per class (in this case two, healhty and not healthy), and one row per instance. For each instance it gives the probability that it is in each class. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.9220339 , 0.93084746, 0.92542373])"
      ]
     },
     "execution_count": 458,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "sgd_clf = SGDClassifier(max_iter=1000, tol=1e-3, loss = 'log', random_state=42)\n",
    "sgd_clf.fit(x_train, y_train,sample_weight= ytrain_sampleweight)\n",
    "\n",
    "cross_val_score(sgd_clf, x_test, y_test, cv=3, scoring='f1_micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = sgd_clf.predict(x_test)\n",
    "print(f\"f1 score (forest): {f1_score(y_test, y_pred, sample_weight= ytest_sampleweight)}\")\n",
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'loss': ['log'],\n",
    "    'penalty': ['elasticnet'],\n",
    "    'alpha': [10 ** x for x in range(-6, 1)],\n",
    "    'l1_ratio': [0, 0.2, 0.5,  0.9, 1],\n",
    "}\n",
    "clf = SGDClassifier(random_state=0, class_weight='balanced')\n",
    "\n",
    "clf_grid = GridSearchCV(estimator=clf, param_grid=param_grid,\n",
    "                                    n_jobs=-1, scoring='f1_micro')\n",
    "clf_grid.fit(x_train,y_train,sample_weight= ytrain_sampleweight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The confusion matrix shows that there are: \n",
    "Its very biased towards the positive values. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss function for logistic regression is the log loss. Therefore I predict the logistic regression to give the best prediction as it gives a higher penalty when the probability is off. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "log_clf = LogisticRegression()\n",
    "log_clf.fit(x_train, y_train,sample_weight= ytrain_sampleweight)\n",
    "cross_val_score(sgd_clf, x_train, y_train, cv=3, scoring='f1_score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = log_clf.predict(x_test)\n",
    "print(f\"f1 score (forest): {f1_score(y_test, y_pred, sample_weight= ytest_sampleweight)}\")\n",
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble Methods\n",
    "https://scikit-learn.org/stable/modules/ensemble.html\n",
    "\n",
    "- Group of decision trees on different random subsets of the data\n",
    "- Combine by voting\n",
    "- Random Forest\n",
    "- Hard voting classifier can be better than the strongest individual classifier\n",
    "- Combining weak learners can lead to a strong learner\n",
    "\n",
    "## Hard Voting\n",
    "I first tried hard voting, then I will see how it compares to soft voting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_moons\n",
    "\n",
    "dt = DecisionTreeClassifier()\n",
    "log_clf = LogisticRegression()\n",
    "rnd_clf = RandomForestClassifier(n_estimators=100)\n",
    "svm_clf = SVC()\n",
    "\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[(\"dt\", dt),(\"lr\", log_clf), (\"rf\", rnd_clf), (\"svc\", svm_clf)],\n",
    "    voting=\"hard\",\n",
    ")\n",
    "voting_clf.fit(x_train, y_train, sample_weight= ytrain_sampleweight)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit on training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for clf in (dt, log_clf, rnd_clf, svm_clf, voting_clf):\n",
    "    clf.fit(x_train, y_train,sample_weight= ytrain_sampleweight)\n",
    "    y_pred = clf.predict(x_test)\n",
    "    print(clf.__class__.__name__, {f1_score(y_test, y_pred,sample_weight= ytest_sampleweight)})\n",
    "    print(clf.__class__.__name__, {accuracy_score(y_test, y_pred, sample_weight= ytest_sampleweight)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Soft Voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soft_svm_clf = SVC(probability=True)\n",
    "\n",
    "soft_voting_clf = VotingClassifier(\n",
    "    estimators=[(\"dt\", dt),(\"lr\", log_clf), (\"rf\", rnd_clf), (\"svc\", soft_svm_clf)],\n",
    "    voting=\"soft\",\n",
    ")\n",
    "soft_voting_clf.fit(x_train, y_train, sample_weight= ytrain_sampleweight)\n",
    "\n",
    "for clf in (dt, log_clf, rnd_clf, svm_clf, soft_voting_clf):\n",
    "    clf.fit(x_train, y_train,sample_weight= ytrain_sampleweight)\n",
    "    y_pred = clf.predict(x_test)\n",
    "    print(clf.__class__.__name__, {f1_score(y_test, y_pred,sample_weight= ytest_sampleweight)})\n",
    "    print(clf.__class__.__name__, {accuracy_score(y_test, y_pred, sample_weight= ytest_sampleweight)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bagging and Pasting\n",
    "\n",
    "- Bagging\n",
    "bootstrap aggregating, sampling with replacement. Creating different bags.\n",
    "- Pasting\n",
    "sampling without replacement\n",
    "\n",
    "- stability\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "bag_clf = BaggingClassifier(\n",
    "    RandomForestClassifier(),\n",
    "    n_estimators=500,\n",
    "    max_samples=100,\n",
    "    bootstrap=True,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "bag_clf.fit(x_train, y_train)\n",
    "y_pred = bag_clf.predict(x_test)\n",
    "accuracy_score(y_test.round(), y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "still very biased towards the negative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boosting\n",
    "- Adaboost\n",
    "- Gradient Boosting\n",
    "\n",
    "For boosting, base estimators are built sequentially and one tries to reduce the bias of the combined estimator. This is combining several weak models to produce a powerful ensemble.\n",
    "\n",
    "- decrease the bias\n",
    "reduce variance in the supervised learning algorithm\n",
    "\n",
    "- Boosting you create a bag and compare the training predicted score with training actual score. You will get some false positive and some false negative, these are saved and added to another bag picked randomly. You train with the ones that you predicted wrong. Pass the false negatives and false positives to the next bag and train again. In this way you are reducing your error. \n",
    "\n",
    "- bar graphs, data analysis, drop some values make data 50 50 so its not biased to one of them.\n",
    "- compare random forest without bagging and without boosting, one should be the best one, boosting probabily. Do bagging with each model individually. AUC / ROC will do this automatically using a threshold. Whichever has most points closer to 1 is the best one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Boosting - Ada Boost\n",
    "\n",
    "adb = AdaBoostClassifier(RandomForestClassifier())\n",
    "adb.fit(x_train,y_train)\n",
    "\n",
    "\n",
    "adb.score(x_test,y_test)\n",
    "\n",
    "adb.score(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = adb.predict(x_test)\n",
    "accuracy_score(y_test.round(), y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning rate and early stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AUC and ROC\n",
    "I want to compare my models AUC to the human scores\n",
    "https://github.com/conversationai/unhealthy-conversations/blob/master/notebooks/AUC_analysis.ipynb\n",
    "\n",
    "Sometimes accuracy is not a good way to evaluate your model. To reduce accuracy we can use bagging. The confusion matrix will show. \n",
    "\n",
    "Plot the ROC of every model and check which one has the highest AUC.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "for model in models:   \n",
    "    preds = model.predict(x_test)\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, preds, pos_label=1)\n",
    "    auc = roc_auc_score(y_test, preds)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(fpr, tpr)\n",
    "    ax.plot([0, 1], [0, 1], color='navy', linestyle='--', label='random')\n",
    "    plt.title(f'AUC: {auc}')\n",
    "    ax.set_xlabel('False positive rate')\n",
    "    ax.set_ylabel('True positive rate')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision-Recall curve\n",
    "\"The precision-recall curve shows the tradeoff between precision and recall for different threshold. A high area under the curve represents both high recall and high precision, where high precision relates to a low false positive rate, and high recall relates to a low false negative rate.\"\n",
    "https://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import auc\n",
    "\n",
    "good_probs = grid_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "precision, recall, _ = precision_recall_curve(y_test, good_probs)\n",
    "auc_score = auc(recall, precision)\n",
    "plt.figure()\n",
    "plt.plot(recall, precision, marker=\".\", label=\"Decision Tree\")\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.title(f\"Precision-Recall for 'Good' wine class; AUC: {auc_score}\")\n",
    "plt.show()\n",
    "plt.close(\"all\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
